{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis: smaller kernel sizes will lead to higher accuracies of classification in fined-tuned image classification\n",
    "# We think this is likely due to the ability of the CNN to detect smaller features between relatively similar images of the same species\n",
    "# Independent Variables: kernel size\n",
    "# Constant Variables: number of layers, types of layers, input shape, epoches, hyperparamaters(depth,padding)\n",
    "# Dependent Variables: Accuracy\n",
    "# Limitations: we are foregoing pure accuracy for experimental reasons - we could get higher accuracy if we tried but we're keeping\n",
    "# things constant for consistancy between models\n",
    "# Currently only using Input, Convolutional, Relu, Pool, and FC/Dense layers. Can add Dense and Dropout if we have time\n",
    "# Reference to www.cs231n.github.io/convolutional-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfu288/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 3. Import libraries and modules\n",
    "import os, cv2, argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from numpy.random import RandomState\n",
    "np.random.seed(123)  # for reproducibility\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(arr1, arr2):\n",
    "    seed = random.randint(0, 1000)\n",
    "    ran = RandomState(seed)\n",
    "    ran.shuffle(arr1)\n",
    "    ran = RandomState(seed)\n",
    "    ran.shuffle(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "TRAIN_DIR = './training/'\n",
    "TEST_DIR = './validation/'\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING IN AND FORMATTING IMAGES\n",
      "COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Setup, Read in images, Preprocess images for training\n",
    "TRAIN_IMG, TRAIN_CLS, TEST_IMG, TEST_CLS = ([] for i in range(4))\n",
    "COLS = ['Label', 'Latin Name', 'Common Name', 'Train Images', 'Validation Images']\n",
    "LABELS = pd.read_csv('./monkey_labels.txt', names=COLS, skiprows=1)\n",
    "CLASSES = [x for x in range(0, len(LABELS))]\n",
    "\n",
    "# read in all images\n",
    "# resizing the images to 100x100 to make training faster\n",
    "print(\"READING IN AND FORMATTING IMAGES\")\n",
    "for x in range(0, len(LABELS)):\n",
    "    train_dir = TRAIN_DIR + LABELS.loc[x,'Label'].strip() + '/'\n",
    "    test_dir = TEST_DIR + LABELS.loc[x,'Label'].strip() + '/'\n",
    "    for file in os.listdir(train_dir):\n",
    "        img = cv2.imread(train_dir + file)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (100, 100))\n",
    "            TRAIN_IMG.append(img)\n",
    "            TRAIN_CLS.append(x)\n",
    "    for file in os.listdir(test_dir):\n",
    "        img = cv2.imread(test_dir + file)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (100, 100))\n",
    "            TEST_IMG.append(img)\n",
    "            TEST_CLS.append(x)\n",
    "print(\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some of the input for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "TRAIN_IMG = np.array(TRAIN_IMG)\n",
    "TEST_IMG = np.array(TEST_IMG)\n",
    "TRAIN_CLS = np.array(TRAIN_CLS)\n",
    "TEST_CLS = np.array(TEST_CLS)\n",
    "\n",
    "# Preprocess images\n",
    "# Reshape them to theanos format (channels, hight, width) # changed to tensorflow\n",
    "# Convert to 0-255 to value in [0-1]\n",
    "# TRAIN_IMG = TRAIN_IMG.reshape(TRAIN_IMG.shape[0], 3, 100, 100)\n",
    "# TEST_IMG = TEST_IMG.reshape(TEST_IMG.shape[0], 3, 100, 100)\n",
    "TRAIN_IMG = TRAIN_IMG.astype('float32')\n",
    "TEST_IMG = TEST_IMG.astype('float32')\n",
    "TRAIN_IMG /= 255\n",
    "TEST_IMG /= 255\n",
    "\n",
    "# Reshape class labels\n",
    "TRAIN_CLS = np_utils.to_categorical(TRAIN_CLS, 10)\n",
    "TEST_CLS = np_utils.to_categorical(TEST_CLS, 10)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffle_data(TRAIN_IMG, TRAIN_CLS)\n",
    "shuffle_data(TEST_IMG, TEST_CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the model - MINST\n",
    "\n",
    "# This model was used in a tutorial for MINST images, but gets a 50% on our test set\n",
    "# after 10 epoches\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(110, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "# model.add(Conv2D(110, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' What does this mean for our Convolutional layer?\\n    Each neuron from the [98x98x110] output volume is attached to a [3x3x3] filter in the input volume.\\n    All the neurons in the same area in the depth column (so 110 of them) actually map to the exact same\\n    filter on the input, but each should have different numerical outputs since each filter is looking for a \\n    different feature.\\n    With [98x98x110] neurons in this Convolutional layer, each neuron with [3x3x3] weighs and 1 bias. This means\\n    that this layer has 29,580,320 paramaters.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Why CNN over NN?\n",
    "    Regular FC NN don't scale well to images - too many weights and too connected - leads to over fitting and long\n",
    "    compute times\n",
    "    Instead of connecting densely like a NN, a CNN layer only connects the neurons in a layer to a small region of \n",
    "    the layer before it. Also allows for reducing the image to a single vector in the end.\n",
    "'''\n",
    "\n",
    "''' What is a Convolutional layer?\n",
    "    A convolutional layer is a 3D layer (vs a 1D layer in a dense) where the depth of the layer is the number of\n",
    "    learnable filters. A filter is a set of weights inna 3d matrix that transveres over the input image. Each filter \n",
    "    is small spatially (width and height wise), but extend the depth of the input (so 3 for our case). A typical \n",
    "    filter may be [5x5x3] - 5 pixels wide and high, 3 deep. During the forward pass, we slide(convolve) each filter \n",
    "    across the width and height of the input and compute dot products between the filter and the input.\n",
    "'''\n",
    "\n",
    "''' What do these filters do?\n",
    "    As we slide these filters across the input image, we build a 2d activation map that gives the responses of that\n",
    "    specific filter at each point in the image. What this means is that the filters scan the input for \"features\" \n",
    "    like edges or areas of high contrast, and the filter will be \"activated\" when it passes its \"feature\" on the \n",
    "    input image. We can see where in the image a feature was found by seeing where on the the 2d activation \n",
    "    map is activated for the filter that is detecting that feature. Convolutional layers deeper in the \n",
    "    architecture might be able to detect more complex attributes in the image like patterns, small objects(eyes, \n",
    "    wheels), etc.\n",
    "'''\n",
    "\n",
    "''' How big is the volume of a Convolutional layer (Height,Width,Depth) given:\n",
    "    INPUT_SIZE(W), FILTER_SIZE(F), NUM_OF_FILTERS, STRIDE(S), PADDING(P)\n",
    "      Padding - It is a HYPERPARAMATER. Pad the input volume with zeros around the border. Generally used to control the\n",
    "        spacial size of the output volumes, usually to match size size of the input volume.\n",
    "      Stride - amount of pixels we shift each filter by when scanning. Stride of 1 means we move filters one pixel \n",
    "        at a time. Larger strides lead to a smaller output volume in the layer\n",
    "      Depth - It is a HYPERPARAMATER. It corresponds to the number of filters we want to use. Each filter looks for \n",
    "        something different in input. \n",
    "    Volume = (Wâˆ’F+2P)/S+1 \n",
    "    Our CNN has an input of 100x100, filter of 3x3, no padding, stride of 1, 110 number of filters. \n",
    "    (100-3) + 1 = 98, so the output layer contains 98x98x110 neurons.\n",
    "'''\n",
    "\n",
    "''' What does this mean for our Convolutional layer?\n",
    "    Each neuron from the [98x98x110] output volume is attached to a [3x3x3] filter in the input volume.\n",
    "    All the neurons in the same area in the depth column (so 110 of them) actually map to the exact same\n",
    "    filter on the input, but each should have different numerical outputs since each filter is looking for a \n",
    "    different feature.\n",
    "    With [98x98x110] neurons in this Convolutional layer, each neuron with [3x3x3] weighs and 1 bias. This means\n",
    "    that this layer has 29,580,320 paramaters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate Model - Simple\n",
    "\n",
    "# Trying to use the simplest CNN possible - From here we can mess with the kernel and see if kernel size helps \n",
    "# with fine tuned classification\n",
    "# Hypothesis: smaller kernel sizes will lead to higher accuracies of classification in fined-tuned image classification\n",
    "# Independent Variables: kernel size\n",
    "# Constant Variables: number of layers, types of layers, input shape, epoches\n",
    "# Dependent Variables: Accuracy\n",
    "# Limitations: we are foregoing pure accuracy for experimental reasons - we could get higher accuracy if we tried but we're keeping\n",
    "# things constant for consistancy between models\n",
    "# Currently only using Input, Convolutional, Relu, Pool, and FC/Dense layers. Can add Dropout(overfitting) if we have time\n",
    "# Reference to www.cs231n.github.io/convolutional-networks\n",
    "# Building a CNN generally requires 4 major steps: Convolution, Pooling, Flattening, Full Connection\n",
    "test_kernel = (2,2)\n",
    "num_filters = 110\n",
    "input_shape=(100,100,3)\n",
    "\n",
    "simple_model = Sequential()\n",
    "# INPUT LAYER - Hold raw pixel values of an image, width 100, height 100, and with 3 color channels\n",
    "# CONVOLUTIONAL LAYER - Compute the output of neurons connected to local regions in the input, each computing the dot \n",
    "#   product vetween their weights and a small region (decided by filter) they are connected to in the input volume\n",
    "simple_model.add(Conv2D(num_filters, test_kernel, input_shape=input_shape))\n",
    "# RELU LAYER - Apply an elemntwise activation function, max(0,x). Leaves the size of the volume unchanged, used to normalize output. Can be added a paramater to the previous layer but seperated for clarity\n",
    "simple_model.add(Activation('relu'))\n",
    "# POOL LAYER - Perform a downsampling operation along the psatial dimentions (width, height) resulting in a smaller volume\n",
    "simple_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "simple_model.add(Flatten()) # Converts and connects multi-dimentional convolutional layer into a 1D feature vector to be used for final classification\n",
    "# FULLY CONNECTED LAYER - Compute class scores, resulting in a volume size of [1x1x10], where each of the 10 numbers represents a class\n",
    "simple_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile MINIST model\n",
    "# print(\"COMPILING MINIST MODEL\")\n",
    "# # Adam - Modified Gradient Decent - learning rate changes as it nears\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPILING SIMPLE MODEL\n",
      "DONE COMPILING MODEL\n"
     ]
    }
   ],
   "source": [
    "# Compile SIMPLE model\n",
    "print(\"COMPILING SIMPLE MODEL\")\n",
    "# Adam - Modified Gradient Decent - learning rate changes as it nears\n",
    "simple_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"DONE COMPILING MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data\n",
    "# print(\"TRAINING FOR {} EPOCHS\".format(EPOCHS)) \n",
    "# history = model.fit(TRAIN_IMG, TRAIN_CLS, batch_size=32, epochs=EPOCHS, verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SIMPLE MODEL FOR 10 EPOCHS using (2, 2) kernel size\n",
      "Train on 878 samples, validate on 220 samples\n",
      "Epoch 1/10\n",
      "878/878 [==============================] - 10s 11ms/step - loss: 8.9159 - acc: 0.1959 - val_loss: 4.4462 - val_acc: 0.2591\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 12s 14ms/step - loss: 2.0605 - acc: 0.3861 - val_loss: 1.6769 - val_acc: 0.4273\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.9968 - acc: 0.7073 - val_loss: 1.3461 - val_acc: 0.5091\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 14s 16ms/step - loss: 0.4539 - acc: 0.8838 - val_loss: 1.2647 - val_acc: 0.5682\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.2001 - acc: 0.9647 - val_loss: 1.2791 - val_acc: 0.5591\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 14s 15ms/step - loss: 0.0817 - acc: 0.9920 - val_loss: 1.1532 - val_acc: 0.6136\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 13s 14ms/step - loss: 0.0413 - acc: 1.0000 - val_loss: 1.4073 - val_acc: 0.5773\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 10s 11ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 1.2501 - val_acc: 0.5955\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 11s 12ms/step - loss: 0.0138 - acc: 1.0000 - val_loss: 1.2698 - val_acc: 0.5955\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 9s 11ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 1.3146 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Train SIMPLE model on the training data\n",
    "# Note: Probably overfitting this model, hit 100% on training before 10 iterations\n",
    "print(\"TRAINING SIMPLE MODEL FOR {} EPOCHS using {} kernel size\".format(EPOCHS, test_kernel)) \n",
    "history = simple_model.fit(TRAIN_IMG, TRAIN_CLS, batch_size=32, epochs=EPOCHS, verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# print(\"SAVE MODEL\")\n",
    "# model.save('test_model.h5')\n",
    "# print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Print a plot of loss and accuracy over epochs and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('test_model.h5')\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "# loss, acc = model.evaluate(TEST_IMG, TEST_CLS, verbose=1)\n",
    "# print(\"Loss: \", loss, \" Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
      "272/272 [==============================] - 1s 3ms/step\n",
      "Loss:  1.1741403691908892  Accuracy:  0.6213235294117647\n"
     ]
    }
   ],
   "source": [
    "# Save and Evaluate the SIMPLE model on the validation data\n",
    "#print(\"SAVE SIMPLE MODEL\")\n",
    "#simple_model.save('models/simple_model.h5')\n",
    "print(history.history.keys())\n",
    "loss, acc = simple_model.evaluate(TEST_IMG, TEST_CLS, verbose=1)\n",
    "print(\"Loss: \", loss, \" Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Append results to the end of a text file\n",
    "# with open(\"results_simple_cnn.txt\", 'a') as f:\n",
    "#     f.write('{{ \"accuracy\":{}, \"epochs\":{}, \"kernel\":\"{}\", \"num_filters\":{}, \"input_shape\":\"{}\" }} \\n'.format(\n",
    "#         acc, EPOCHS, test_kernel, num_filters, input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(TEST_IMG)):\\n    img = TEST_IMG[i]\\n    cls = TEST_CLS[i]\\n    img = np.array([img])\\n    prediction = model.predict(img, verbose=1, steps=1)\\n    print\\n    print \"Class: \", cls\\n    print \"Prediction: \", prediction[0]\\n    max_index = np.argmax(prediction[0])\\n    print \"Predicted Class index: \", max_index\\n    print \"Prediction Correct: \", True if cls[max_index] == 1. else False\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict images\n",
    "# TODO - Print mispredicted images, the label it predicted, and the correct label\n",
    "'''\n",
    "for i in range(len(TEST_IMG)):\n",
    "    img = TEST_IMG[i]\n",
    "    cls = TEST_CLS[i]\n",
    "    img = np.array([img])\n",
    "    prediction = model.predict(img, verbose=1, steps=1)\n",
    "    print\n",
    "    print \"Class: \", cls\n",
    "    print \"Prediction: \", prediction[0]\n",
    "    max_index = np.argmax(prediction[0])\n",
    "    print \"Predicted Class index: \", max_index\n",
    "    print \"Prediction Correct: \", True if cls[max_index] == 1. else False\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
